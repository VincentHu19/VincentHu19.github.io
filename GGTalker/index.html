<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Talking head, Audio-driven, NeRF, SyncTalk">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GGTalker</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vincenthu19.github.io/">Wentao Hu</a><sup>1</sup>,</span>
            <span class="author-block">
              <strong>Shunkai Li</strong><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://ziqiaopeng.github.io/">Ziqiao Peng</a><sup>3</sup>,</span>
            <span class="author-block">
              <strong>Haoxian Zhang</strong><sup>2</sup>,
            </span>
            <span class="author-block">
              <strong>Fan Shi</strong><sup>2</sup>,
            </span>
            <br>
            <span class="author-block">
              <strong>Xiaoqiang Liu</strong><sup>2</sup>,
            </span>
            <span class="author-block">
              <strong>Pengfei Wan</strong><sup>2</sup>,
            </span>
            <span class="author-block">
              <strong>Di Zhang</strong><sup>2</sup>,
            </span>
            <span class="author-block">
              <strong>Hui Tian</strong><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beijing University of Posts and Telecommunications,</span>
            <br>
            <span class="author-block"><sup>2</sup>Kuaishou Technology,</span>
            <span class="author-block"><sup>3</sup>Renmin University of China</span>

          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper(Coming Soon)</span>
                </a>
              </span>

              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/aWVsM3H0xOE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
<!--               <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ziqiaopeng/SyncTalk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay loop controls playsinline height="100%">
        <source src="./static/videos/synctalk-arxiv.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <strong>GGTalker</strong>  learns 3D head priors from only 2D in-the-wild image collections. We supervise 3D Gaussians using 
        2D data in a self-supervised manner. During inference, our model reconstructs one-shot high-quality 3D head avatar from a single image.
      </h2>
    </div>
  </div>
</section> -->

  
<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Video -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/aWVsM3H0xOE?si=_qIdixW5OM5oZWp4"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Creating high-quality, generalizable speech-driven 3D talking heads remains a persistent challenge. 
            Previous methods achieve satisfactory results for fixed viewpoints and small-scale audio variations, 
            but they struggle with large head rotations and out-of-distribution (OOD) audio. Moreover, 
            they are constrained by the need for time-consuming, identity-specific training. We believe the core 
            issue lies in the lack of sufficient 3D priors, which limits the extrapolation capabilities of 
            synthesized talking heads. To address this, we propose GGTalker, which synthesizes talking heads through 
            a combination of generalizable priors and identity-specific adaptation. We introduce a two-stage Prior-Adaptation 
            training strategy to learn Gaussian head priors and adapt to individual characteristics. We train Audio-Expression 
            and Expression-Visual priors to capture the universal patterns of lip movements and the general distribution of head textures. 
            During the Customized Adaptation, individual speaking styles and texture details are precisely modeled. 
            Additionally, we introduce a color MLP to generate fine-grained, motion-aligned textures and a Body Inpainter to 
            blend rendered results with the background, producing indistinguishable, photorealistic video frames. 
            Comprehensive experiments show that GGTalker achieves state-of-the-art performance in rendering quality, 
            3D consistency, lip-sync accuracy, and training efficiency.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    
    <!-- Proposed Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Method</h2>
        <div class="content has-text-justified">
          <img src="./static/videos/pipeline.png" alt="GGAvatars">
          <p>
            <br>
            Overview of GGTalker. We introduce a two-stage Prior-Adaptation approach for talking head synthesis. 
            We use a conditional diffusion transformer to predict plausible facial movements from an audio sequence. 
            For texture generation, we leverage the UV layout of the canonical mesh and use an Identity-Gaussian Generator to predict a UV Gaussian map from a single reference image, 
            initializing a reasonable Gaussian distribution for the head. We adapt well-trained priors to target individual’s speaking style and texture details. 
            We further introduce a color MLP and a Body Inpainter to generate motion-aligned fine textures and lifelike results.
          </p>
          <!-- <p>
            
          </p>
          <p>
            
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Proposed Method. -->  
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
  @article{hu2025ggtalker,
    title={GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation}, 
  }
</code></pre>
  </div>
</section>
 



<footer class="footer">
  <div class="container">
      <div class="content has-text-centered">
          <a class="icon-link" href="https://arxiv.org/abs/2311.17590">
              <i class="fas fa-file-pdf"></i>
          </a>
          <a class="icon-link" href="https://github.com/ZiqiaoPeng/SyncTalk" class="external-link" disabled>
              <i class="fab fa-github"></i>
          </a>
      </div>
      <div class="columns is-centered">
          <div class="column is-8">
              <div class="content">
                  <p style="text-align:center">
                    This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  </p>
                  <p style="text-align:center">
                    Website source code based on the <a
                    href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
                  </p>

              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
